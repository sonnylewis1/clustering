# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_XKS3nXyVV71InVO5rMjWfqZo9CSlyOA

# ***IMPORTING OF THE NECESSARY LIBRARIES***
"""

import nltk
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from gensim.models import LdaModel
from gensim.corpora import Dictionary
import gensim
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.decomposition import PCA
from sklearn.metrics import cohen_kappa_score
from gensim.models.coherencemodel import CoherenceModel
from sklearn.metrics import silhouette_score
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn import mixture
from gensim.models import LdaMulticore
from sklearn.metrics import homogeneity_score
from sklearn.metrics import completeness_score
from collections import Counter

"""***DOWNLOADING RELEVANT RESOURCES FROM NLTK***"""

nltk.download('gutenberg')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

"""***CHECKING THE CONTENT OF GUTENBERG***"""

nltk.corpus.gutenberg.fileids()

"""***ASSIGNING THE SELECTED TEXTS TO VARIABLES***"""

text1 =  nltk.corpus.gutenberg.raw('austen-emma.txt')
text2 = nltk.corpus.gutenberg.raw('bible-kjv.txt')
text3 = nltk.corpus.gutenberg.raw('whitman-leaves.txt')
text4 = nltk.corpus.gutenberg.raw('milton-paradise.txt')
text5 = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')
text6 = nltk.corpus.gutenberg.raw('edgeworth-parents.txt')
text7 = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')

print(text1)

"""***VIEWING THE NUMBER OF WORDS IN EACH TEXT***"""

number_words_texts=[(len(nltk.word_tokenize(text1)),'austen-emma'),(len(nltk.word_tokenize(text2)),'bible-kjv'), (len(nltk.word_tokenize(text3)),'whitman-leaves'), 
                    (len(nltk.word_tokenize(text4)),'milton-paradise'), (len(nltk.word_tokenize(text5)),'melville-moby_dick'), 
                    (len(nltk.word_tokenize(text6)),'edgeworth-parents'), (len(nltk.word_tokenize(text7)),'chesterton-thursday')]
print(number_words_texts)

n_o_w = pd.DataFrame(number_words_texts)
n_o_w = n_o_w.rename(columns={0: "Number of words", 1: "Text"})
print(n_o_w)

sns.barplot(n_o_w['Text'],n_o_w['Number of words'],label="Number of Words")
plt.xticks(rotation=90)
plt.show()

"""***TOKENIZING THE SELECTED TEXTS INTO SENTENCES***"""

sent1 = nltk.sent_tokenize(text1)
sent2 = nltk.sent_tokenize(text2)
sent3 = nltk.sent_tokenize(text3)
sent4 = nltk.sent_tokenize(text4)
sent5 = nltk.sent_tokenize(text5)
sent6 = nltk.sent_tokenize(text6)
sent7 = nltk.sent_tokenize(text7)

print(sent3)

"""## ***PREPROCESSING OF THE DATA***

***DEFINING A FUNCTION (word_list) TO REMOVE STOPWORDS, NUMBERS, PUNCTUATIONS AS WELL AS LEMMATIZE***
"""

wordlemmatize = WordNetLemmatizer()

def word_list(sent):
    sentn = ''
    sentn = sentn.join(sent)
    sentn=sentn.replace('.',' ').replace(',',' ').replace('!',' ').replace('?',' ').replace('--',' ').replace('-',' ').replace(';',' ').replace("'",' ').replace('"',' ').replace("_",' ').replace(':',' ').replace('(',' ').replace(')',' ').replace('0',' ').replace('1',' ').replace('2',' ').replace('3',' ').replace('4',' ').replace('5',' ').replace('6',' ').replace('7',' ').replace('8',' ').replace('9',' ')#REPLACING OF UNWANTED CHARACTERS
    words = nltk.word_tokenize(sentn) #TOKENIZING THE SENTENCES INTO WORDS
    stop_words = set(stopwords.words('english'))

    wordlen = []
    words = [w.lower() for w in words if not w in stop_words] #REMOVAL OF STOPWORDS FROM THE TEXT 
    for word in words:
      wordsv = wordlemmatize.lemmatize(word, pos='v') #LEMMATIZING OF THE TEXT BASED ON VERBS
      wordsa = wordlemmatize.lemmatize(wordsv, pos='a') #LEMMATIZING OF THE TEXT BASED ON ADJECTIVES
      words = wordlemmatize.lemmatize(wordsa, pos='n') #LEMMATIZING OF THE TEXT BASED ON NOUNS
      wordlen.append(words) #APPENDING THE WORDS AFTER THE REMOVAL OF STOPWORDS, UNWANTED PUNTUATIONS, SYMBOLS, NUMBERS AND LEMMANTIZING 
    return wordlen

"""***APPLYING THE FUNCTION (word_list) TO THE TOKENIZED SENTENCES***"""

words1 = word_list(sent1)
words2 = word_list(sent2)
words3 = word_list(sent3)
words4 = word_list(sent4)
words5 = word_list(sent5)
words6 = word_list(sent6)
words7 = word_list(sent7)

word_count = Counter(words1)
print(word_count.most_common(10))

word_count = Counter(words2)
print(word_count.most_common(10))

word_count = Counter(words3)
print(word_count.most_common(10))

word_count = Counter(words4)
print(word_count.most_common(10))

word_count = Counter(words5)
print(word_count.most_common(10))

word_count = Counter(words6)
print(word_count.most_common(10))

word_count = Counter(words7)
print(word_count.most_common(10))

"""***CHECKING FOR THE LENGHT OF THE TEXTS AFTER THE FIRST FUNTION WAS APPLIED***"""

number_words_texts=[(len(words1),'austen-emma'),(len(words2),'bible-kjv'),(len(words3),'whitman-leaves'), (len(words4), 'milton-paradise'),
                    (len(words5),'melville-moby_dick'), (len(words6), 'edgeworth-parents'), (len(words7),'chesterton-thursday')]
n_o_w = pd.DataFrame(number_words_texts)
n_o_w = n_o_w.rename(columns={0: "Number of words", 1: "Text"})
print(n_o_w)

"""***VISUALIZATION OF THE LENGHT OF THE TEXTS***"""

sns.barplot(n_o_w['Text'],n_o_w['Number of words'],label="Number of Words")
plt.xticks(rotation=90)
plt.show()

"""***DEFINING ANOTHER FUNTION (random_sample) TO ASSIGN 150 WORDS TO A DOCUMENT AND PICK 200 RANDOM DOCUMENTS FROM EACH TEXT***"""

def random_sample(list_words):
  count = 0
  str = ''
  for word in list_words:
    if count<150:
      str = str + word
      str = str+' '
      count = count + 1
    else:
      str = str+'###'
      count = 0
  strr = str.split('###')
  return random.sample(strr,200)

"""***APPLYING OF THE SECOND FUNCTION (random_sample) TO THE PROCESSED TEXTS***"""

r_str1 = random_sample(words1)
r_str2 = random_sample(words2)
r_str3 = random_sample(words3)
r_str4 = random_sample(words4)
r_str5 = random_sample(words5)
r_str6 = random_sample(words6)
r_str7 = random_sample(words7)

print(r_str1[4])

"""***ASSIGNING THE NAME OF THE AUTHOR OF EACH DOCUMENT TO THE RESPECTIVE DOCUMENT***"""

labeled_names = ([(sent, 'austen') for sent in r_str1] + [(sent, 'bible') for sent in r_str2]+[(sent, 'whitman') for sent in r_str3]+
                 [(sent, 'milton') for sent in r_str4]+[(sent, 'melville') for sent in r_str5]+[(sent, 'edgeworth') for sent in r_str6]+
								         [(sent, 'chesterton') for sent in r_str7])
pd.DataFrame(labeled_names)

"""***SHUFFLING THE LABELED NAMES AND VIEWING THEM IN A DATAFRAME USING PANDAS***"""

random.shuffle(labeled_names)
labeled = pd.DataFrame(labeled_names)

"""***LABELING OF THE COLUMNS OF THE DATAFRAME***"""

labeled = labeled.rename(columns={0: "Text", 1: "Author"})
labeled.head()

"""***SPLITTING OF THE DATASET***"""

X = labeled['Text'].values
y = labeled['Author'].values

pd.DataFrame(X).head()

"""***TRANSFORMING OF THE AUTHOR COLUMN***"""

labelencoder = LabelEncoder()
y = labelencoder.fit_transform(y)

pd.DataFrame(y).head()

"""## **FEATURE ENGINEERING**
***TRANSFORMATION OF THE TEXT USING BAG OF WORDS***
"""

count = CountVectorizer(min_df=3, analyzer='word', ngram_range=(1,2)) #CONSIDERING BOTH BIGRAMS AND UNIGRAMS, IGNORING WORDS THAT HAVE A DOCUMENT FREQUENCY OF LESS THAN 3.
bow = count.fit_transform(X)
array_bow = bow.toarray()

Bow_feature_names = count.get_feature_names()

X_Bow = pd.DataFrame(bow.toarray(), columns=Bow_feature_names) #VIEWING IN FORM OF A DATAFRAME
X_Bow

"""***TRANSFORMATION OF THE TEXT USING TF-IDF***"""

tf = TfidfVectorizer(analyzer='word',min_df= 3, ngram_range=(1, 2))
Tfid = tf.fit_transform(X)
array_tfid = Tfid.toarray()

tfid_feature_names = tf.get_feature_names()
X_Tfid = pd.DataFrame(Tfid.toarray(), columns=tfid_feature_names)
X_Tfid

"""## ***LDA***

***FINDING THE BEST NUMBER OF TOPICS TO BE USED***
"""

Dict = labeled['Text'].map(word_list)

#CREATING A DICTIONARY AND A CORPUS

dictionary = gensim.corpora.Dictionary(Dict)
corpus = [dictionary.doc2bow(doc)for doc in Dict]

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model,model_list, coherence_values

model,model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=Dict, start=2, limit=40, step=6)

limit=40; start=2; step=6;
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()

"""***TRANSFORMATION OF THE TEXT USING LDA FROM BOW***"""

lda = LDA(n_components=25, n_jobs=-1)
array_lda_Bow = lda.fit_transform(X_Bow)

X_lda_Bow = pd.DataFrame(array_lda_Bow)
X_lda_Bow

"""***TRANSFORMATION OF THE TEXT USING LDA FROM TFID***"""

lda = LDA(n_components=25, n_jobs=-1)
array_lda_Tfid = lda.fit_transform(X_Tfid)

X_lda_Tfid = pd.DataFrame(array_lda_Tfid)
X_lda_Tfid

"""***CONCATENATION OF MODELS***"""

concat_lda = np.concatenate((array_tfid,array_lda_Tfid), axis = 1)

pd.DataFrame(concat_lda)

concat_lda = pd.concat([X_Tfid, X_lda_Tfid], axis=1, sort=False)

concat_lda

concat_bow = np.concatenate((array_bow,array_lda_Bow), axis = 1)

pd.DataFrame(concat_bow)

"""## ***MODELLING***

***K MEANS***
"""

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_Bow)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss, marker='o', markersize = 7)
plt.title('The Elbow Method')
plt.xlabel('Number of centroids')
plt.ylabel('WCSS')
plt.show()

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_Tfid)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss, marker='o', markersize = 7)
plt.title('The Elbow Method')
plt.xlabel('Number of centroids')
plt.ylabel('WCSS')
plt.show()

kmeans = KMeans(n_clusters=7, init='k-means++', max_iter=300, n_init=10, random_state=0)
pca = PCA(n_components=2, random_state=0)

pred_bow = kmeans.fit_predict(array_bow)

reduced_features = pca.fit_transform(array_bow)
reduced_cluster = pca.transform(kmeans.cluster_centers_)

plt.scatter(reduced_features[:,0], reduced_features[:, 1], s = 20, c = pred_bow)
plt.scatter(reduced_cluster[:, 0], reduced_cluster[:, 1], s = 50, c = 'black', label = 'Centroids')

doc_per_cluster = pd.DataFrame(pred_bow)
doc_per_cluster = doc_per_cluster.rename(columns={0: "Label"})
doc_per_cluster['Label'].value_counts()

pred_tfid = kmeans.fit_predict(array_tfid)

reduced_features = pca.fit_transform(array_tfid)
reduced_cluster = pca.transform(kmeans.cluster_centers_)

plt.scatter(reduced_features[:,0], reduced_features[:, 1], s = 20, c = pred_tfid)
plt.scatter(reduced_cluster[:, 0], reduced_cluster[:, 1], s = 50, c = 'black', label = 'Centroids')

doc_per_cluster = pd.DataFrame(pred_tfid)
doc_per_cluster = doc_per_cluster.rename(columns={0: "Label"})
doc_per_cluster['Label'].value_counts()

pred_ldabow = kmeans.fit_predict(array_lda_Bow)

reduced_featureslda = pca.fit_transform(array_lda_Bow)
reduced_cluster = pca.transform(kmeans.cluster_centers_)

plt.scatter(reduced_featureslda[:,0], reduced_featureslda[:, 1], s = 20, c = pred_ldabow)
plt.scatter(reduced_cluster[:, 0], reduced_cluster[:, 1], s = 50, c = 'black', label = 'Centroids')

doc_per_cluster = pd.DataFrame(pred_ldabow)
doc_per_cluster = doc_per_cluster.rename(columns={0: "Label"})
doc_per_cluster['Label'].value_counts()

pred_ldatfid = kmeans.fit_predict(array_lda_Tfid)

reduced_featureslda = pca.fit_transform(array_lda_Tfid)
reduced_cluster = pca.transform(kmeans.cluster_centers_)

plt.scatter(reduced_featureslda[:,0], reduced_featureslda[:, 1], s = 20, c = pred_ldatfid)
plt.scatter(reduced_cluster[:, 0], reduced_cluster[:, 1], s = 50, c = 'black', label = 'Centroids')

doc_per_cluster = pd.DataFrame(pred_ldatfid)
doc_per_cluster = doc_per_cluster.rename(columns={0: "Label"})
doc_per_cluster['Label'].value_counts()

pred_ldaconcat = kmeans.fit_predict(concat_lda)

reduced_featuresconcat_lda = pca.fit_transform(concat_lda)
reduced_cluster = pca.transform(kmeans.cluster_centers_)

plt.scatter(reduced_featuresconcat_lda[:,0], reduced_featuresconcat_lda[:, 1], s = 20, c = pred_ldaconcat)
plt.scatter(reduced_cluster[:, 0], reduced_cluster[:, 1], s = 50, c = 'black', label = 'Centroids')

doc_per_cluster = pd.DataFrame(pred_ldaconcat)
doc_per_cluster = doc_per_cluster.rename(columns={0: "Label"})
doc_per_cluster['Label'].value_counts()

pred_bowconcat = kmeans.fit_predict(concat_bow)

reduced_featuresconcat_bow = pca.fit_transform(concat_bow)
reduced_cluster = pca.transform(kmeans.cluster_centers_)

plt.scatter(reduced_featuresconcat_bow[:,0], reduced_featuresconcat_bow[:, 1], s = 20, c = pred_bowconcat)
plt.scatter(reduced_cluster[:, 0], reduced_cluster[:, 1], s = 50, c = 'black', label = 'Centroids')

doc_per_cluster = pd.DataFrame(pred_bowconcat)
doc_per_cluster = doc_per_cluster.rename(columns={0: "Label"})
doc_per_cluster['Label'].value_counts()

"""***AGGLOMERATIVE CLUSTERING***"""

# create dendrogram
dendrogram = sch.dendrogram(sch.linkage(array_bow, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=7, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
agg_bow = hc.fit_predict(array_bow)
labels1 = hc.labels_

# create dendrogram
dendrogram = sch.dendrogram(sch.linkage(array_tfid, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=7, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
agg_tfid = hc.fit_predict(array_tfid)
labels2 = hc.labels_

# create dendrogram
dendrogram = sch.dendrogram(sch.linkage(array_lda_Bow, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=7, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
agg_lda_bow = hc.fit_predict(array_lda_Bow)
labels3 = hc.labels_

# create dendrogram
dendrogram = sch.dendrogram(sch.linkage(array_lda_Tfid, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=7, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
agg_lda_tfid = hc.fit_predict(array_lda_Tfid)
labels4 = hc.labels_

# create dendrogram
dendrogram = sch.dendrogram(sch.linkage(concat_lda, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=7, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
agg_lda_concat = hc.fit_predict(concat_lda)

# create dendrogram
dendrogram = sch.dendrogram(sch.linkage(concat_bow, method='ward'))
# create clusters
hc = AgglomerativeClustering(n_clusters=7, affinity = 'euclidean', linkage = 'ward')
# save clusters for chart
agg_bow_concat = hc.fit_predict(concat_bow)

# using Gaussian mixture model(GMM) as a ANN model for clusterring
gmm = mixture.GaussianMixture(n_components=7,covariance_type='full')
gmm.fit(array_bow)
pred_bow_gmm = gmm.fit_predict(array_bow)

pca3 = PCA(n_components=2, random_state=0)
reduced_features3 = pca3.fit_transform(array_bow)
plt.scatter(reduced_features3[:,0], reduced_features3[:,1], c = pred_bow)

#using LDA as a Topic Model
for idx, topic in model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

"""# ***MODEL EVALUATION***

***SILHOUETTE***
"""

silhouette_score(array_bow,pred_bow)

silhouette_score(concat_lda,pred_ldaconcat)

silhouette_score(array_tfid,pred_tfid)

silhouette_score(array_lda_Bow,pred_ldabow)

silhouette_score(array_lda_Tfid,pred_ldatfid)

silhouette_score(concat_bow,pred_bowconcat) #using kmeans (bow)

silhouette_score(array_bow,pred_bow_gmm) #using gmm(bow)

silhouette_score(array_bow,labels1) #using agglomerative(bow)

silhouette_score(array_tfid,labels2) #using agglomerative(tfid)

silhouette_score(array_lda_Bow,labels3) #using agglomerative(lda-bow)

silhouette_score(array_lda_Tfid,labels4) #using agglomerative(bow)

#calculating the homogenity score for different models

homogeneity_score(y, pred_ldatfid)

homogeneity_score(y, pred_tfid)

homogeneity_score(y, pred_ldabow)

homogeneity_score(y, pred_ldaconcat)

homogeneity_score(y,pred_bow_gmm)

homogeneity_score(y,labels1) #using agglomerative(bow)

homogeneity_score(y,labels2) #using agglomerative(tfid)

homogeneity_score(y,labels3) #using agglomerative(lda using bow)

homogeneity_score(y,labels4) #using agglomerative(lda using tfid)

#calculating the completeness score for different models

completeness_score(y, pred_ldatfid )

completeness_score(y, pred_tfid)

completeness_score(y,pred_ldabow)

completeness_score(y, pred_ldaconcat)

completeness_score(y,pred_bow_gmm)

completeness_score(y, labels1)

completeness_score(y,labels2)

completeness_score(y,labels3)

completeness_score(y,labels4)

"""***KAPPA EVALUATION***"""

cohen_kappa_score(pred_ldabow,labels3)#COMPARING KMean USING lda-BOW with Agglomerative using lda-BOW

"""***ERROR ANALYSIS***"""

print("Top terms per cluster TFID:")
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = tf.get_feature_names()
for i in range(0,7):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

print("Top terms per cluster BOW:")
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
terms = count.get_feature_names()
for i in range(0,7):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print
